{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "authorship_tag": "ABX9TyNolMapTUyXfvM9X0isrbmf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitHubAndyLee2020/ExchangeAgent/blob/main/stock_exchange_decision_transformer_agent_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stock Exchange Decision Transformer Agent v1"
      ],
      "metadata": {
        "id": "jNe_An4qDJt7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNXhverPDFaQ"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install gym-anytrading\n",
        "!pip install accelerate -U"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handling Training Data"
      ],
      "metadata": {
        "id": "VLE1pk-qFZsm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym_anytrading\n",
        "\n",
        "df = gym_anytrading.datasets.STOCKS_GOOGL.copy()"
      ],
      "metadata": {
        "id": "hHDB_wl3HdjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(20)"
      ],
      "metadata": {
        "id": "F2qnD6sXI-gO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail(20)"
      ],
      "metadata": {
        "id": "w9zaLw6MJA_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "from datasets import load_dataset\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "fwnlEIBfD71W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "dataset = load_dataset(\"techandy42/ppo-200K-collected-dataset-steps-1\")"
      ],
      "metadata": {
        "id": "egB3t313EjSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "qYqbuM666P9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Before Reshaping"
      ],
      "metadata": {
        "id": "9R_zmAG4JT-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Take the first sample from the dataset\n",
        "first_sample = dataset['train'][0]\n",
        "\n",
        "first_sample_values = {key: value for key, value in first_sample.items()}\n",
        "\n",
        "for key, value in first_sample_values.items():\n",
        "    print(f\"{key}: {value}\")"
      ],
      "metadata": {
        "id": "3ebFBHJ2Hu0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take the last sample from the dataset\n",
        "last_sample = dataset['train'][-1]\n",
        "\n",
        "last_sample_values = {key: value for key, value in last_sample.items()}\n",
        "\n",
        "for key, value in last_sample_values.items():\n",
        "    print(f\"{key}: {value}\")"
      ],
      "metadata": {
        "id": "aeJUyDLJIIjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a random sample from the dataset\n",
        "random_sample = random.choice(dataset['train'])\n",
        "\n",
        "random_sample_values = {key: value for key, value in random_sample.items()}\n",
        "\n",
        "for key, value in random_sample_values.items():\n",
        "    print(f\"{key}: {value}\")"
      ],
      "metadata": {
        "id": "qjhRArHnQtAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to reshape the observation\n",
        "def reshape_observation(example):\n",
        "    example['observation'] = example['observation'][0][-1]\n",
        "    return example\n",
        "\n",
        "# Apply the function to each item in the dataset\n",
        "dataset = dataset.map(reshape_observation)"
      ],
      "metadata": {
        "id": "sUGY6lKPQN6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### After Reshaping"
      ],
      "metadata": {
        "id": "asSwIaZcJWH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Take the first sample from the dataset\n",
        "first_sample = dataset['train'][0]\n",
        "\n",
        "first_sample_values = {key: value for key, value in first_sample.items()}\n",
        "\n",
        "for key, value in first_sample_values.items():\n",
        "    print(f\"{key}: {value}\")"
      ],
      "metadata": {
        "id": "cAdqu6jBJXjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take the last sample from the dataset\n",
        "last_sample = dataset['train'][-1]\n",
        "\n",
        "last_sample_values = {key: value for key, value in last_sample.items()}\n",
        "\n",
        "for key, value in last_sample_values.items():\n",
        "    print(f\"{key}: {value}\")"
      ],
      "metadata": {
        "id": "LxJtHyFuJZiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a random sample from the dataset\n",
        "random_sample = random.choice(dataset['train'])\n",
        "\n",
        "random_sample_values = {key: value for key, value in random_sample.items()}\n",
        "\n",
        "for key, value in random_sample_values.items():\n",
        "    print(f\"{key}: {value}\")"
      ],
      "metadata": {
        "id": "U6CJ9q4eFY9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Action Type Count"
      ],
      "metadata": {
        "id": "KE6VlNs-J1CN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Action Type Analysis\n",
        "action_type_count = {\n",
        "    \"short_no_reward\": 0,\n",
        "    \"short_reward\": 0,\n",
        "    \"long\": 0,\n",
        "}\n",
        "\n",
        "def count_action_type(example):\n",
        "    if example['action'][0] == 0:\n",
        "      if example['reward'][0] == 0:\n",
        "        action_type_count[\"short_no_reward\"] += 1\n",
        "      else:\n",
        "        action_type_count[\"short_reward\"] += 1\n",
        "    else:\n",
        "      action_type_count[\"long\"] += 1\n",
        "    return example\n",
        "\n",
        "dataset = dataset.map(count_action_type)"
      ],
      "metadata": {
        "id": "dLOdYpEj25n_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key, value in action_type_count.items():\n",
        "    print(f\"{key}: {value}\")"
      ],
      "metadata": {
        "id": "Eaw9cHxH3dty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define DataCollator"
      ],
      "metadata": {
        "id": "zidtfvZfFqkL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from dataclasses import dataclass\n",
        "from transformers import DecisionTransformerConfig, DecisionTransformerModel, Trainer, TrainingArguments"
      ],
      "metadata": {
        "id": "a6Y3xvg2Fw0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class DecisionTransformerGymDataCollator:\n",
        "    return_tensors: str = \"pt\"\n",
        "    max_len: int = 10 #subsets of the episode we use for training\n",
        "    state_dim: int = 2  # size of state space\n",
        "    act_dim: int = 1  # size of action space\n",
        "    max_ep_len: int = 2324 # max episode length in the dataset\n",
        "    scale: float = 1.0  # normalization of rewards/returns\n",
        "    state_mean: np.array = None  # to store state means\n",
        "    state_std: np.array = None  # to store state stds\n",
        "    p_sample: np.array = None  # a distribution to take account trajectory lengths\n",
        "    n_traj: int = 0 # to store the number of trajectories in the dataset\n",
        "\n",
        "    def __init__(self, dataset) -> None:\n",
        "        self.dataset = dataset\n",
        "        # calculate dataset stats for normalization of states\n",
        "        states = []\n",
        "        traj_lens = []\n",
        "        for obs in dataset[\"observation\"]:\n",
        "            states.extend(obs)\n",
        "            traj_lens.append(len(obs))\n",
        "        self.n_traj = len(traj_lens)\n",
        "        states = np.vstack(states)\n",
        "        self.state_mean, self.state_std = np.mean(states, axis=0), np.std(states, axis=0) + 1e-6\n",
        "\n",
        "        traj_lens = np.array(traj_lens)\n",
        "        self.p_sample = traj_lens / sum(traj_lens)\n",
        "\n",
        "    def _discount_cumsum(self, x, gamma):\n",
        "        discount_cumsum = np.zeros_like(x)\n",
        "        discount_cumsum[-1] = x[-1]\n",
        "        for t in reversed(range(x.shape[0] - 1)):\n",
        "            discount_cumsum[t] = x[t] + gamma * discount_cumsum[t + 1]\n",
        "        return discount_cumsum\n",
        "\n",
        "    def __call__(self, features):\n",
        "        batch_size = len(features)\n",
        "        # this is a bit of a hack to be able to sample of a non-uniform distribution\n",
        "        batch_inds = np.random.choice(\n",
        "            np.arange(self.n_traj),\n",
        "            size=batch_size,\n",
        "            replace=True,\n",
        "            p=self.p_sample,  # reweights so we sample according to timesteps\n",
        "        )\n",
        "        # a batch of dataset features\n",
        "        s, a, r, d, rtg, timesteps, mask = [], [], [], [], [], [], []\n",
        "\n",
        "        for ind in batch_inds:\n",
        "            # for feature in features:\n",
        "            feature = self.dataset[int(ind)]\n",
        "            si = random.randint(0, len(feature[\"reward\"]) - 1)\n",
        "\n",
        "            # get sequences from dataset\n",
        "            s.append(np.array(feature[\"observation\"][si : si + self.max_len]).reshape(1, -1, self.state_dim))\n",
        "            a.append(np.array(feature[\"action\"][si : si + self.max_len]).reshape(1, -1, self.act_dim))\n",
        "            r.append(np.array(feature[\"reward\"][si : si + self.max_len]).reshape(1, -1, 1))\n",
        "\n",
        "            d.append(np.array(feature[\"done\"][si : si + self.max_len]).reshape(1, -1))\n",
        "            timesteps.append(np.arange(si, si + s[-1].shape[1]).reshape(1, -1))\n",
        "            timesteps[-1][timesteps[-1] >= self.max_ep_len] = self.max_ep_len - 1  # padding cutoff\n",
        "            rtg.append(\n",
        "                self._discount_cumsum(np.array(feature[\"reward\"][si:]), gamma=1.0)[\n",
        "                    : s[-1].shape[1]   # TODO check the +1 removed here\n",
        "                ].reshape(1, -1, 1)\n",
        "            )\n",
        "            if rtg[-1].shape[1] < s[-1].shape[1]:\n",
        "                print(\"if true\")\n",
        "                rtg[-1] = np.concatenate([rtg[-1], np.zeros((1, 1, 1))], axis=1)\n",
        "\n",
        "            # padding and state + reward normalization\n",
        "            tlen = s[-1].shape[1]\n",
        "            s[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, self.state_dim)), s[-1]], axis=1)\n",
        "            s[-1] = (s[-1] - self.state_mean) / self.state_std\n",
        "            a[-1] = np.concatenate(\n",
        "                [np.ones((1, self.max_len - tlen, self.act_dim)) * -10.0, a[-1]],\n",
        "                axis=1,\n",
        "            )\n",
        "            r[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, 1)), r[-1]], axis=1)\n",
        "            d[-1] = np.concatenate([np.ones((1, self.max_len - tlen)) * 2, d[-1]], axis=1)\n",
        "            rtg[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, 1)), rtg[-1]], axis=1) / self.scale\n",
        "            timesteps[-1] = np.concatenate([np.zeros((1, self.max_len - tlen)), timesteps[-1]], axis=1)\n",
        "            mask.append(np.concatenate([np.zeros((1, self.max_len - tlen)), np.ones((1, tlen))], axis=1))\n",
        "\n",
        "        s = torch.from_numpy(np.concatenate(s, axis=0)).float()\n",
        "        a = torch.from_numpy(np.concatenate(a, axis=0)).float()\n",
        "        r = torch.from_numpy(np.concatenate(r, axis=0)).float()\n",
        "        d = torch.from_numpy(np.concatenate(d, axis=0))\n",
        "        rtg = torch.from_numpy(np.concatenate(rtg, axis=0)).float()\n",
        "        timesteps = torch.from_numpy(np.concatenate(timesteps, axis=0)).long()\n",
        "        mask = torch.from_numpy(np.concatenate(mask, axis=0)).float()\n",
        "\n",
        "        return {\n",
        "            \"states\": s,\n",
        "            \"actions\": a,\n",
        "            \"rewards\": r,\n",
        "            \"returns_to_go\": rtg,\n",
        "            \"timesteps\": timesteps,\n",
        "            \"attention_mask\": mask,\n",
        "        }"
      ],
      "metadata": {
        "id": "Cld9qI9gFgwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Model"
      ],
      "metadata": {
        "id": "wYReBIUnTNZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainableDT(DecisionTransformerModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.action_probs_list = []\n",
        "\n",
        "    def forward(self, **kwargs):\n",
        "        output = super().forward(**kwargs)\n",
        "\n",
        "        # Binary action prediction\n",
        "        action_preds = output[1]\n",
        "        action_targets = kwargs[\"actions\"]\n",
        "        attention_mask = kwargs[\"attention_mask\"]\n",
        "        act_dim = action_preds.shape[2]\n",
        "\n",
        "        action_preds = action_preds.reshape(-1, act_dim)[attention_mask.reshape(-1) > 0]\n",
        "        action_targets = action_targets.reshape(-1, act_dim)[attention_mask.reshape(-1) > 0]\n",
        "\n",
        "        # Sigmoid activation to get probabilities\n",
        "        action_probs = torch.sigmoid(action_preds)\n",
        "        self.action_probs_list.append(action_probs)\n",
        "        # Thresholding to get binary actions\n",
        "        action_preds_binary = (action_probs > 0.5).float()\n",
        "\n",
        "        # Binary Cross-Entropy Loss\n",
        "        loss = F.binary_cross_entropy(action_probs, action_targets)\n",
        "\n",
        "        return {\"loss\": loss}\n",
        "\n",
        "    def original_forward(self, **kwargs):\n",
        "        return super().forward(**kwargs)"
      ],
      "metadata": {
        "id": "1hTeV_LAKQc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Model"
      ],
      "metadata": {
        "id": "epsHTEQCTPRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "collator = DecisionTransformerGymDataCollator(dataset[\"train\"])\n",
        "\n",
        "config = DecisionTransformerConfig(state_dim=collator.state_dim, act_dim=collator.act_dim)\n",
        "model = TrainableDT(config)"
      ],
      "metadata": {
        "id": "lmkBK-XxKi-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"output/\",\n",
        "    remove_unused_columns=False,\n",
        "    num_train_epochs=140,\n",
        "    per_device_train_batch_size=64,\n",
        "    learning_rate=1e-4,\n",
        "    weight_decay=1e-4,\n",
        "    warmup_ratio=0.1,\n",
        "    optim=\"adamw_torch\",\n",
        "    max_grad_norm=0.25,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    data_collator=collator,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "Z0Xkhe0JKm7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine action_probs values across training steps\n",
        "for i in range(0, len(model.action_probs_list), 100):\n",
        "  print(model.action_probs_list[i])"
      ],
      "metadata": {
        "id": "OZ-zkCkKCwiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Model"
      ],
      "metadata": {
        "id": "cv6xyGfHTQpP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function that gets an action from the model using autoregressive prediction with a window of the previous 20 timesteps.\n",
        "def get_action(model, states, actions, rewards, returns_to_go, timesteps):\n",
        "    # Reshape inputs\n",
        "    states = states.reshape(1, -1, model.config.state_dim)\n",
        "    actions = actions.reshape(1, -1, model.config.act_dim)\n",
        "    returns_to_go = returns_to_go.reshape(1, -1, 1)\n",
        "    timesteps = timesteps.reshape(1, -1)\n",
        "\n",
        "    # Apply window of the previous 20 timesteps\n",
        "    states = states[:, -model.config.max_length :]\n",
        "    actions = actions[:, -model.config.max_length :]\n",
        "    returns_to_go = returns_to_go[:, -model.config.max_length :]\n",
        "    timesteps = timesteps[:, -model.config.max_length :]\n",
        "    padding = model.config.max_length - states.shape[1]\n",
        "\n",
        "    device = model.device  # Get the device from the model\n",
        "\n",
        "    # Pad all tokens to sequence length\n",
        "    zero_pad = torch.zeros(padding, device=device)\n",
        "    attention_mask = torch.cat((zero_pad, torch.ones(states.shape[1], device=device)), dim=0)\n",
        "    attention_mask = attention_mask.to(dtype=torch.long).reshape(1, -1)\n",
        "\n",
        "    # Ensure all tensors are moved to the same device before concatenation\n",
        "    zero_tensor_state = torch.zeros((1, padding, model.config.state_dim), device=device)\n",
        "    zero_tensor_action = torch.zeros((1, padding, model.config.act_dim), device=device)\n",
        "    zero_tensor_return = torch.zeros((1, padding, 1), device=device)\n",
        "    zero_tensor_timestep = torch.zeros((1, padding), dtype=torch.long, device=device)\n",
        "\n",
        "    states = torch.cat((zero_tensor_state, states.to(device)), dim=1).float()\n",
        "    actions = torch.cat((zero_tensor_action, actions.to(device)), dim=1).float()\n",
        "    returns_to_go = torch.cat((zero_tensor_return, returns_to_go.to(device)), dim=1).float()\n",
        "    timesteps = torch.cat((zero_tensor_timestep, timesteps.to(device)), dim=1)\n",
        "\n",
        "    state_preds, action_preds, return_preds = model.original_forward(\n",
        "        states=states,\n",
        "        actions=actions,\n",
        "        returns_to_go=returns_to_go,\n",
        "        timesteps=timesteps,\n",
        "        attention_mask=attention_mask,\n",
        "        return_dict=False,\n",
        "    )\n",
        "\n",
        "    # Apply sigmoid to the last timestep's action prediction and threshold to get a binary action\n",
        "    last_action_pred = torch.sigmoid(action_preds[0, -1])\n",
        "    binary_action = (last_action_pred > 0.5).float()\n",
        "\n",
        "    return binary_action"
      ],
      "metadata": {
        "id": "dqS6Ut5ETWPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import gym_anytrading"
      ],
      "metadata": {
        "id": "iSmWYPEKWfM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env_id = \"stocks-v0\"\n",
        "\n",
        "df = gym_anytrading.datasets.STOCKS_GOOGL.copy()\n",
        "\n",
        "window_size = 10\n",
        "start_index = window_size\n",
        "end_index = len(df)\n",
        "\n",
        "env = gym.make(\n",
        "    env_id,\n",
        "    df=df,\n",
        "    window_size=window_size,\n",
        "    frame_bound=(start_index, end_index)\n",
        ")\n",
        "\n",
        "print(\"observation_space:\", env.observation_space)"
      ],
      "metadata": {
        "id": "idgWGHUhWAyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_ep_len = 2324\n",
        "device = \"cuda\"\n",
        "scale = 1.0  # normalization for rewards/returns\n",
        "TARGET_RETURN = 2000 / scale  # evaluation is conditioned on a return of 12000, scaled accordingly\n",
        "\n",
        "state_mean = collator.state_mean.astype(np.float32)\n",
        "state_std = collator.state_std.astype(np.float32)\n",
        "print(state_mean)\n",
        "\n",
        "state_dim = 20\n",
        "act_dim = 1\n",
        "# Create the decision transformer model\n",
        "\n",
        "state_mean = torch.from_numpy(state_mean).to(device=device)\n",
        "state_std = torch.from_numpy(state_std).to(device=device)"
      ],
      "metadata": {
        "id": "5Rujn3gpWomd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Interact with the environment\n",
        "episode_return, episode_length = 0, 0\n",
        "state = env.reset()\n",
        "state = state[0].flatten()\n",
        "target_return = torch.tensor(TARGET_RETURN, device=device, dtype=torch.float32).reshape(1, 1)\n",
        "states = torch.from_numpy(state).reshape(1, state_dim).to(device=device, dtype=torch.float32)\n",
        "actions = torch.zeros((0, act_dim), device=device, dtype=torch.float32)\n",
        "rewards = torch.zeros(0, device=device, dtype=torch.float32)\n",
        "model_action_type_count = {\n",
        "    \"short_no_reward\": 0,\n",
        "    \"short_reward\": 0,\n",
        "    \"long\": 0,\n",
        "}\n",
        "\n",
        "timesteps = torch.tensor(0, device=device, dtype=torch.long).reshape(1, 1)\n",
        "for t in range(max_ep_len):\n",
        "    actions = torch.cat([actions, torch.zeros((1, act_dim), device=device)], dim=0)\n",
        "    rewards = torch.cat([rewards, torch.zeros(1, device=device)])\n",
        "\n",
        "    action = get_action(\n",
        "        model,\n",
        "        (states - state_mean) / state_std,\n",
        "        actions,\n",
        "        rewards,\n",
        "        target_return,\n",
        "        timesteps,\n",
        "    )\n",
        "    actions[-1] = action\n",
        "    action = action.detach().cpu().numpy()\n",
        "    # Choose random action for each step\n",
        "    # action = np.array([random.choice([0, 1])])\n",
        "\n",
        "    state, reward, done, _, info = env.step(action)\n",
        "    state = state.flatten()\n",
        "\n",
        "    if action[0] == 0:\n",
        "        if reward == 0:\n",
        "            model_action_type_count[\"short_no_reward\"] += 1\n",
        "        else:\n",
        "            model_action_type_count[\"short_reward\"] += 1\n",
        "    elif action[0] == 1:\n",
        "        model_action_type_count[\"long\"] += 1\n",
        "\n",
        "    cur_state = torch.from_numpy(state).to(device=device).reshape(1, state_dim)\n",
        "    states = torch.cat([states, cur_state], dim=0)\n",
        "    rewards[-1] = reward\n",
        "\n",
        "    pred_return = target_return[0, -1] - (reward / scale)\n",
        "    target_return = torch.cat([target_return, pred_return.reshape(1, 1)], dim=1)\n",
        "    timesteps = torch.cat([timesteps, torch.ones((1, 1), device=device, dtype=torch.long) * (t + 1)], dim=1)\n",
        "\n",
        "    episode_return += reward\n",
        "    episode_length += 1\n",
        "\n",
        "    if done:\n",
        "        break"
      ],
      "metadata": {
        "id": "c0nsJjfUXm0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key, value in model_action_type_count.items():\n",
        "    print(f\"{key}: {value}\")"
      ],
      "metadata": {
        "id": "b0r-9GRH5Y4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Episode return: {episode_return}\")\n",
        "print(f\"Episode length: {episode_length}\")"
      ],
      "metadata": {
        "id": "ife8FiCYxkTA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}