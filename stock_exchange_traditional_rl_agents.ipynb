{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "authorship_tag": "ABX9TyPhcWGQ6f8MkbmZNS2qZ8LR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitHubAndyLee2020/ExchangeGPT/blob/main/stock_exchange_traditional_rl_agents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stock Exchange Project Traditional RL Agents\n",
        "\n"
      ],
      "metadata": {
        "id": "WjvTwtatmPpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym-anytrading stable-baselines3 huggingface_sb3 datasets quantstats"
      ],
      "metadata": {
        "id": "UVYnQim7mz1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "55cwwZORmM_N"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import gymnasium as gym\n",
        "import gym_anytrading\n",
        "\n",
        "from stable_baselines3 import A2C, PPO\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env_id = \"stocks-v0\"\n",
        "\n",
        "df = gym_anytrading.datasets.STOCKS_GOOGL.copy()\n",
        "\n",
        "window_size = 10\n",
        "start_index = window_size\n",
        "end_index = len(df)\n",
        "\n",
        "env = gym.make(\n",
        "    env_id,\n",
        "    df=df,\n",
        "    window_size=window_size,\n",
        "    frame_bound=(start_index, end_index)\n",
        ")\n",
        "\n",
        "print(\"observation_space:\", env.observation_space)"
      ],
      "metadata": {
        "id": "ffO71TyTm9Il"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_stats(reward_over_episodes):\n",
        "    \"\"\"  Print Reward  \"\"\"\n",
        "\n",
        "    avg = np.mean(reward_over_episodes)\n",
        "    min = np.min(reward_over_episodes)\n",
        "    max = np.max(reward_over_episodes)\n",
        "\n",
        "    print (f'Min. Reward          : {min:>10.3f}')\n",
        "    print (f'Avg. Reward          : {avg:>10.3f}')\n",
        "    print (f'Max. Reward          : {max:>10.3f}')\n",
        "\n",
        "    return min, avg, max\n",
        "\n",
        "\n",
        "# ProgressBarCallback for model.learn()\n",
        "class ProgressBarCallback(BaseCallback):\n",
        "\n",
        "    def __init__(self, check_freq: int, verbose: int = 1):\n",
        "        super().__init__(verbose)\n",
        "        self.check_freq = check_freq\n",
        "\n",
        "    def _on_training_start(self) -> None:\n",
        "        \"\"\"\n",
        "        This method is called before the first rollout starts.\n",
        "        \"\"\"\n",
        "        self.progress_bar = tqdm(total=self.model._total_timesteps, desc=\"model.learn()\")\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        if self.n_calls % self.check_freq == 0:\n",
        "            self.progress_bar.update(self.check_freq)\n",
        "        return True\n",
        "\n",
        "    def _on_training_end(self) -> None:\n",
        "        \"\"\"\n",
        "        This event is triggered before exiting the `learn()` method.\n",
        "        \"\"\"\n",
        "        self.progress_bar.close()\n",
        "\n",
        "\n",
        "# TRAINING + TEST\n",
        "def train_test_model(model, env, seed, total_num_episodes, total_learning_timesteps=10_000):\n",
        "    \"\"\" if model=None then execute 'Random actions' \"\"\"\n",
        "\n",
        "    # reproduce training and test\n",
        "    print('-' * 80)\n",
        "    obs = env.reset(seed=seed)\n",
        "    torch.manual_seed(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    vec_env = None\n",
        "\n",
        "    if model is not None:\n",
        "        print(f'model {type(model)}')\n",
        "        print(f'policy {type(model.policy)}')\n",
        "        # print(f'model.learn(): {total_learning_timesteps} timesteps ...')\n",
        "\n",
        "        # custom callback for 'progress_bar'\n",
        "        model.learn(total_timesteps=total_learning_timesteps, callback=ProgressBarCallback(100))\n",
        "        # model.learn(total_timesteps=total_learning_timesteps, progress_bar=True)\n",
        "        # ImportError: You must install tqdm and rich in order to use the progress bar callback.\n",
        "        # It is included if you install stable-baselines with the extra packages: `pip install stable-baselines3[extra]`\n",
        "\n",
        "        vec_env = model.get_env()\n",
        "        obs = vec_env.reset()\n",
        "    else:\n",
        "        print (\"RANDOM actions\")\n",
        "\n",
        "    reward_over_episodes = []\n",
        "\n",
        "    tbar = tqdm(range(total_num_episodes))\n",
        "\n",
        "    for episode in tbar:\n",
        "\n",
        "        if vec_env:\n",
        "            obs = vec_env.reset()\n",
        "        else:\n",
        "            obs, info = env.reset()\n",
        "\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            if model is not None:\n",
        "                action, _states = model.predict(obs)\n",
        "                obs, reward, done, info = vec_env.step(action)\n",
        "            else: # random\n",
        "                action = env.action_space.sample()\n",
        "                obs, reward, terminated, truncated, info = env.step(action)\n",
        "                done = terminated or truncated\n",
        "\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        reward_over_episodes.append(total_reward)\n",
        "\n",
        "        if episode % 10 == 0:\n",
        "            avg_reward = np.mean(reward_over_episodes)\n",
        "            tbar.set_description(f'Episode: {episode}, Avg. Reward: {avg_reward:.3f}')\n",
        "            tbar.update()\n",
        "\n",
        "    tbar.close()\n",
        "    avg_reward = np.mean(reward_over_episodes)\n",
        "\n",
        "    return reward_over_episodes\n",
        "\n",
        "# TEST ONLY\n",
        "def test_model(model, env, total_num_episodes, deterministic=False):\n",
        "    \"\"\"Test a trained model or execute 'Random actions' if model is None.\"\"\"\n",
        "\n",
        "    reward_over_episodes = []\n",
        "\n",
        "    for episode in tqdm(range(total_num_episodes), desc=\"Testing\"):\n",
        "        obs = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            if model is not None:\n",
        "                # Using the model to predict the action\n",
        "                action, _states = model.predict(obs, deterministic=deterministic)\n",
        "                action = [action]  # Ensure action is in the correct format\n",
        "            else:\n",
        "                # Random actions\n",
        "                action = [env.action_space.sample()]\n",
        "\n",
        "            obs, rewards, dones, infos = env.step(action)\n",
        "            reward = rewards[0]  # Extract the scalar reward value\n",
        "            done = dones[0]  # Extract the scalar done value\n",
        "            total_reward += reward\n",
        "\n",
        "        reward_over_episodes.append(total_reward)\n",
        "\n",
        "    return reward_over_episodes"
      ],
      "metadata": {
        "id": "FFtgdNCpm_cY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset, DatasetDict\n",
        "from tqdm import tqdm\n",
        "\n",
        "def collect_dataset(model, env, total_num_episodes, deterministic=False):\n",
        "    \"\"\"Test a trained model or execute 'Random actions' if model is None, and collect trajectories as a HuggingFace dataset.\"\"\"\n",
        "\n",
        "    # Initialize lists for each component of the trajectory\n",
        "    observations, actions, rewards, dones = [], [], [], []\n",
        "\n",
        "    for episode in tqdm(range(total_num_episodes), desc=\"Collecting Dataset\"):\n",
        "        obs = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            if model is not None:\n",
        "                # Using the model to predict the action\n",
        "                action, _states = model.predict(obs, deterministic=deterministic)\n",
        "            else:\n",
        "                # Random actions\n",
        "                action = env.action_space.sample()\n",
        "\n",
        "            new_obs, reward, done, _ = env.step(action)\n",
        "\n",
        "            # Store each component of the step in their respective lists\n",
        "            observations.append(obs)\n",
        "            actions.append(action)\n",
        "            rewards.append(reward)\n",
        "            dones.append(done)\n",
        "\n",
        "            obs = new_obs\n",
        "\n",
        "    # Create a Hugging Face dataset from the collected data\n",
        "    data_dict = {\n",
        "        'observation': observations,\n",
        "        'action': actions,\n",
        "        'reward': rewards,\n",
        "        'done': dones\n",
        "    }\n",
        "    train_dataset = Dataset.from_dict(data_dict)\n",
        "\n",
        "    # Create a DatasetDict with the 'train' split\n",
        "    dataset_dict = DatasetDict({\n",
        "        'train': train_dataset\n",
        "    })\n",
        "\n",
        "    return dataset_dict"
      ],
      "metadata": {
        "id": "FzWpM3H5vDPB"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 42  # random seed\n",
        "total_num_episodes = 50 # Switch to 50 during production, use 2 during testing to reduce runtime\n",
        "\n",
        "print (\"env_name                 :\", env_id)\n",
        "print (\"seed                     :\", seed)\n",
        "\n",
        "# INIT matplotlib\n",
        "plot_settings = {}\n",
        "plot_data = {'x': [i for i in range(1, total_num_episodes + 1)]}\n",
        "\n",
        "# Random actions\n",
        "model = None\n",
        "total_learning_timesteps = 0\n",
        "rewards = train_test_model(model, env, seed, total_num_episodes, total_learning_timesteps)\n",
        "min, avg, max = print_stats(rewards)\n",
        "class_name = f'Random actions'\n",
        "label = f'Avg. {avg:>7.2f} : {class_name}'\n",
        "plot_data['rnd_rewards'] = rewards\n",
        "plot_settings['rnd_rewards'] = {'label': label}\n",
        "\n",
        "# Specify the learning time steps here\n",
        "# The following time steps will take around 2.5 h to train\n",
        "learning_timesteps_list_in_K = [25, 50, 100, 200, 500, 1000]\n",
        "\n",
        "# RL Algorithms: https://stable-baselines3.readthedocs.io/en/master/guide/algos.html\n",
        "model_class_list = [A2C, PPO]\n",
        "models = {}\n",
        "\n",
        "for timesteps in learning_timesteps_list_in_K:\n",
        "    total_learning_timesteps = timesteps * 1000\n",
        "    step_key = f'{timesteps}K'\n",
        "\n",
        "    for model_class in model_class_list:\n",
        "        policy_dict = model_class.policy_aliases\n",
        "        # https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html\n",
        "        # MlpPolicy or MlpLstmPolicy\n",
        "        policy = policy_dict.get('MlpPolicy')\n",
        "        if policy is None:\n",
        "            policy = policy_dict.get('MlpLstmPolicy')\n",
        "        # print ('policy:', policy, 'model_class:', model_class)\n",
        "\n",
        "        try:\n",
        "            model = model_class(policy, env, verbose=0)\n",
        "            class_name = type(model).__qualname__\n",
        "            plot_key = f'{class_name}_rewards_'+step_key\n",
        "            rewards = train_test_model(model, env, seed, total_num_episodes, total_learning_timesteps)\n",
        "            min, avg, max, = print_stats(rewards)\n",
        "            label = f'Avg. {avg:>7.2f} : {class_name} - {step_key}'\n",
        "            plot_data[plot_key] = rewards\n",
        "            plot_settings[plot_key] = {'label': label}\n",
        "            if timesteps not in models.keys():\n",
        "                models[timesteps] = {}\n",
        "            models[timesteps][class_name] = model\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR: {str(e)}\")\n",
        "            continue"
      ],
      "metadata": {
        "id": "-FoYZuwPnCMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.DataFrame(plot_data)\n",
        "\n",
        "sns.set_style('whitegrid')\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "for key in plot_data:\n",
        "    if key == 'x':\n",
        "        continue\n",
        "    label = plot_settings[key]['label']\n",
        "    line = plt.plot('x', key, data=data, linewidth=1, label=label)\n",
        "\n",
        "plt.xlabel('episode')\n",
        "plt.ylabel('reward')\n",
        "plt.title('Random vs. SB3 Agents')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Gb3ZTDrNnIMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for timesteps in models.keys():\n",
        "    for class_name in models[timesteps].keys():\n",
        "        model = models[timesteps][class_name]\n",
        "        model.save(f\"{class_name}-{env_id}-{timesteps}K\")"
      ],
      "metadata": {
        "id": "F0_dT94sYpU5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "\n",
        "class RenderCompatibleEnvWrapper(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "\n",
        "    def render(self, mode='rgb_array'):\n",
        "        # Assuming a dummy image size of 64x64 with 3 channels (RGB)\n",
        "        dummy_image = np.zeros((64, 64, 3), dtype=np.uint8)\n",
        "        return dummy_image\n",
        "\n",
        "# Wrap your environment\n",
        "wrapped_env = lambda: RenderCompatibleEnvWrapper(gym.make(\n",
        "    env_id,\n",
        "    df=df,\n",
        "    window_size=window_size,\n",
        "    frame_bound=(start_index, end_index)\n",
        "))\n",
        "\n",
        "# Create the DummyVecEnv with the wrapped environment\n",
        "eval_env = DummyVecEnv([wrapped_env])\n",
        "\n",
        "# Set render_mode and training flag\n",
        "eval_env.render_mode = \"rgb_array\"\n",
        "eval_env.training = False"
      ],
      "metadata": {
        "id": "gk9f1uV2jLsJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving Models to HuggingFace"
      ],
      "metadata": {
        "id": "2cFizqjaXCJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n",
        "!git config --global credential.helper store"
      ],
      "metadata": {
        "id": "2fqMYbeGW1gs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_sb3 import package_to_hub\n",
        "\n",
        "for timesteps in models.keys():\n",
        "    for class_name in models[timesteps].keys():\n",
        "        model = models[timesteps][class_name]\n",
        "        package_to_hub(\n",
        "            model=model,\n",
        "            model_name=f\"{class_name}-{env_id}-{timesteps}K\",\n",
        "            model_architecture=class_name,\n",
        "            env_id=env_id,\n",
        "            eval_env=eval_env,\n",
        "            repo_id=f\"techandy42/{class_name}-{env_id}-{timesteps}K\",\n",
        "            commit_message=\"Initial commit\",\n",
        "        )"
      ],
      "metadata": {
        "id": "EhtTUR6WXPYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Models from HuggingFace"
      ],
      "metadata": {
        "id": "gg8f_a7akkCG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from huggingface_sb3 import load_from_hub\n",
        "from stable_baselines3 import A2C, PPO\n",
        "\n",
        "model_class_and_name_list = [(A2C, 'A2C'), (PPO, 'PPO')]\n",
        "learning_timesteps = [25, 50, 100, 200, 500, 1000]\n",
        "\n",
        "loaded_models = {}\n",
        "\n",
        "for timesteps in learning_timesteps:\n",
        "    for model_class, class_name in model_class_and_name_list:\n",
        "        model_id = f\"{class_name}-{env_id}-{timesteps}K\"\n",
        "        print(f\"Loading {model_id}...\")\n",
        "        checkpoint = load_from_hub(\n",
        "            repo_id=f\"techandy42/{model_id}\",\n",
        "            filename=f\"{model_id}.zip\",\n",
        "        )\n",
        "        if timesteps not in loaded_models.keys():\n",
        "            loaded_models[timesteps] = {}\n",
        "        loaded_models[timesteps][class_name] = model_class.load(checkpoint)"
      ],
      "metadata": {
        "id": "6Oxx_QojkpJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "\n",
        "# Create a test vectorized environment for testing all models\n",
        "test_env = DummyVecEnv([lambda: gym.make(\n",
        "    env_id,\n",
        "    df=df,\n",
        "    window_size=window_size,\n",
        "    frame_bound=(start_index, end_index)\n",
        ")])\n",
        "\n",
        "# Set render_mode and training flag for the testing environment\n",
        "test_env.render_mode = \"rgb_array\"\n",
        "test_env.training = False"
      ],
      "metadata": {
        "id": "8p2zemy_my9W"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the models and the random policy\n",
        "total_num_episodes = 50  # Adjust this number as needed\n",
        "rewards_dict = {}\n",
        "\n",
        "# Evaluate loaded models\n",
        "for timesteps in loaded_models.keys():\n",
        "    for class_name in loaded_models[timesteps].keys():\n",
        "        model_label = f\"{class_name} - {timesteps}K\"\n",
        "        model = loaded_models[timesteps][class_name]\n",
        "        rewards_dict[model_label] = test_model(model, test_env, total_num_episodes)\n",
        "\n",
        "# Evaluate random policy\n",
        "rewards_dict[\"RandomPolicy\"] = test_model(None, test_env, total_num_episodes)"
      ],
      "metadata": {
        "id": "ieDYUfcfHlKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data for plotting\n",
        "data = pd.DataFrame(rewards_dict)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot and annotate each model's performance\n",
        "for key in rewards_dict:\n",
        "    mean_reward = np.mean(rewards_dict[key])\n",
        "    plt.plot(data[key], label=f\"{key} (Avg: {mean_reward:.2f})\")\n",
        "\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total Reward')\n",
        "plt.title('Performance of Different Policies')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rfsw1vrPt504"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quantstat Analysis"
      ],
      "metadata": {
        "id": "fqU4MbMT1EMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = loaded_models[200]['PPO']"
      ],
      "metadata": {
        "id": "ifFor6rn1Fif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "window_size = 10\n",
        "start_index = window_size\n",
        "end_index = len(df)\n",
        "\n",
        "quant_env = gym.make(\n",
        "    'stocks-v0',\n",
        "    df=df,\n",
        "    window_size=window_size,\n",
        "    frame_bound=(start_index, end_index)\n",
        ")\n",
        "\n",
        "print(\"observation_space:\", quant_env.observation_space)"
      ],
      "metadata": {
        "id": "PddL4FU319qA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gym_anytrading.envs import Actions\n",
        "import quantstats as qs\n",
        "\n",
        "action_stats = {Actions.Sell: 0, Actions.Buy: 0}\n",
        "\n",
        "observation, info = quant_env.reset(seed=2023)\n",
        "\n",
        "while True:\n",
        "    # action = env.action_space.sample()\n",
        "    action, _states = best_model.predict(observation)\n",
        "\n",
        "    action_stats[Actions(action)] += 1\n",
        "    observation, reward, terminated, truncated, info = quant_env.step(action)\n",
        "    done = terminated or truncated\n",
        "\n",
        "    # quant_env.render()\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "quant_env.close()\n",
        "\n",
        "print(\"action_stats:\", action_stats)\n",
        "print(\"info:\", info)"
      ],
      "metadata": {
        "id": "BZBHDvFF1dVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(16, 6))\n",
        "quant_env.unwrapped.render_all()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "E2_6c0uZ2MuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qs.extend_pandas()\n",
        "\n",
        "net_worth = pd.Series(quant_env.unwrapped.history['total_profit'], index=df.index[start_index+1:end_index])\n",
        "returns = net_worth.pct_change().iloc[1:]\n",
        "\n",
        "qs.reports.full(returns)\n",
        "qs.reports.html(returns, output='SB3_a2c_quantstats.html')"
      ],
      "metadata": {
        "id": "O6Nzw_PI2Q23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Collecting Dataset"
      ],
      "metadata": {
        "id": "vY4iAR4t39Mt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = loaded_models[200]['PPO']"
      ],
      "metadata": {
        "id": "Id2T5i8P3-j7"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "\n",
        "# Create a test vectorized environment for testing all models\n",
        "collect_dataset_env = DummyVecEnv([lambda: gym.make(\n",
        "    env_id,\n",
        "    df=df,\n",
        "    window_size=window_size,\n",
        "    frame_bound=(start_index, end_index)\n",
        ")])\n",
        "\n",
        "# Set render_mode and training flag for the testing environment\n",
        "collect_dataset_env.render_mode = \"rgb_array\"\n",
        "collect_dataset_env.training = False"
      ],
      "metadata": {
        "id": "aqd6kI884bF5"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_num_episodes = 500  # Adjust this number as needed\n",
        "dataset = collect_dataset(best_model, collect_dataset_env, total_num_episodes)"
      ],
      "metadata": {
        "id": "wyb2KJWJ4lBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "NOT9yCwS5Y4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a random sample from the dataset\n",
        "random_sample = random.choice(dataset['train'])\n",
        "\n",
        "random_sample_values = {key: value[0] for key, value in random_sample.items()}\n",
        "\n",
        "for key, value in random_sample_values.items():\n",
        "    print(f\"{key}: {value}\")"
      ],
      "metadata": {
        "id": "8HRemeBP5xwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n",
        "!git config --global credential.helper store"
      ],
      "metadata": {
        "id": "DgBLww1K8FlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.push_to_hub(\"ppo-200K-collected-dataset-steps-500\")"
      ],
      "metadata": {
        "id": "3sKLyBAk8LN5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}